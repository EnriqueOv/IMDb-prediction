{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDb Logistic Reg",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WFzqjPaAc2mO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Binarized categorical data according to the genre\n",
        "def binarizegenre(col, genre):\n",
        "    if col == genre:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "2wt7hO7Vdy39"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movieset = pd.read_csv('BagOfWords.csv')\n",
        "x = movieset.drop(['genre1','genre2','genre3','titles','average rating','num of ratings'], axis = 1).copy()\n",
        "y = movieset['genre1'].copy()\n",
        "posgenres = movieset.genre1.unique()"
      ],
      "metadata": {
        "id": "ldjS0Urfd7si"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mincount = int(input(\"What was the max value you put in the IMDb scraper? \"))\n",
        "\n",
        "for genre in posgenres:\n",
        "  if movieset['genre1'].value_counts()[genre] != mincount:\n",
        "    movieset = movieset[(movieset.genre1 != genre)]\n"
      ],
      "metadata": {
        "id": "3ogbWtUkd-gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gentest = input(\"Select which genre to do the test (type \\\"all\\\" to do a test with all genre classifiers): \")\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = .3)\n",
        "\n",
        "\n",
        "  #Makes copies of the test and training sets, to binarize the y_training sets\n",
        "  for genres in posgenres:\n",
        "    globals()[f'{genres}_ytest'] = y_test\n",
        "    globals()[f'{genres}_ytest'] = globals()[f'{genres}_ytest'].apply(lambda i: binarizegenre(i, genres))\n",
        "    globals()[f'{genres}_ytrain'] = y_train\n",
        "    globals()[f'{genres}_ytrain'] = globals()[f'{genres}_ytrain'].apply(lambda i: binarizegenre(i, genres))\n",
        "\n",
        "  #Model creation\n",
        "  if gentest != \"all\":\n",
        "    clf = LogisticRegression(class_weight='balanced').fit(X_train,globals()[f'{gentest}_ytrain'])\n",
        "  else:\n",
        "    clf = LogisticRegression(class_weight='balanced').fit(X_train,y_train)\n",
        "\n",
        "  #Model prediction\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  #Scores\n",
        "  if gentest != \"all\":\n",
        "    acc = metrics.accuracy_score(globals()[f'{gentest}_ytest'],y_pred)\n",
        "    f1 = metrics.f1_score(globals()[f'{gentest}_ytest'],y_pred)\n",
        "    prec = metrics.precision_score(globals()[f'{gentest}_ytest'],y_pred)\n",
        "    recall = metrics.recall_score(globals()[f'{gentest}_ytest'],y_pred)\n",
        "  else:\n",
        "    acc = metrics.accuracy_score(y_test,y_pred)\n",
        "    f1 = metrics.f1_score(y_test,y_pred,average='macro')\n",
        "    prec = metrics.precision_score(y_test,y_pred,average = 'macro')\n",
        "    recall = metrics.recall_score(y_test,y_pred,average = 'macro')\n",
        "\n",
        "  #Print\n",
        "  print(f'Accuracy Score: {round(acc,2)}')\n",
        "  print(f'F1 Score: {round(f1,2)}')\n",
        "  print(f'Precision Score: {round(prec,2)}')\n",
        "  print(f'Recall: {round(recall,2)}')\n",
        "\n"
      ],
      "metadata": {
        "id": "Pgdsu21veBgb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}