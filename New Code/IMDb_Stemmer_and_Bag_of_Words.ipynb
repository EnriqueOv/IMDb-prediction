{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fshr1RwJMjY"
      },
      "outputs": [],
      "source": [
        "#Use conda if using jupyter, use pip if using colab\n",
        "#!conda install contractions\n",
        "#!conda install pattern\n",
        "\n",
        "#!pip install contractions\n",
        "#!pip install pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYzgyRZ_U0yA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "import contractions\n",
        "from pattern.text.en import singularize\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k5uTsbsFUZku"
      },
      "outputs": [],
      "source": [
        "movieset = pd.read_csv('movieset.csv')\n",
        "moviesetstem = movieset.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDsfljBnJigU"
      },
      "source": [
        "#Removing Contractions\n",
        "Removing apostrophes from words and separating them <br>\n",
        "didn't -> did not <br>\n",
        "you're -> you are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uERRAJzRI6tS"
      },
      "outputs": [],
      "source": [
        "#Copy column into series\n",
        "setsummaries = movieset['summaries'].copy()\n",
        "\n",
        "#Turn sentences into array of words, expand contractions\n",
        "x = 0\n",
        "for summary in setsummaries:\n",
        "  setsummaries[x] = summary.split(' ')\n",
        "  y = 0\n",
        "  for word in setsummaries[x]:\n",
        "    setsummaries[x][y] = contractions.fix(word)\n",
        "    y += 1\n",
        "  x += 1\n",
        "\n",
        "#Turn array of words back into sentences\n",
        "x = 0\n",
        "for summary in setsummaries:\n",
        "  setsummaries[x] = \" \".join(summary)\n",
        "  x += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuWMYYgGKHkY"
      },
      "source": [
        "# Removing punctuation\n",
        "Self explanatory, removing punctuation to make words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SysDZ6afJHcM"
      },
      "outputs": [],
      "source": [
        "#Cleaning up words (Removing punctuation)\n",
        "\n",
        "x = 0\n",
        "for sentence in setsummaries:\n",
        "  setsummaries[x] = sentence.replace('\\'s', '')\\\n",
        "                      .replace(',','')\\\n",
        "                      .replace('-',' ')\\\n",
        "                      .replace('.','')\\\n",
        "                      .replace('!','')\\\n",
        "                      .replace('?','')\\\n",
        "                      .replace('\\'','')\\\n",
        "                      .replace(':','')\\\n",
        "                      .replace('(','')\\\n",
        "                      .replace(')','')\\\n",
        "                      .replace('\\\"','')\n",
        "  x = x + 1\n",
        "\n",
        "#Cleaning up words (Removing numbers)\n",
        "x = 0\n",
        "for sentence in setsummaries:\n",
        "  sentence.replace('007','doubleOseven') #This number has meaning (James Bond films)\n",
        "  nonum = re.sub('[0-9]+[a-z]+', '', sentence) #removes positions (1st, 4th, etc)\n",
        "  setsummaries[x] = re.sub('[0-9]+','',nonum) #removes series of numbers\n",
        "  sentence.replace('doubleOseven','007') #Bringing back the 007\n",
        "  x += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPl-fHC2LdxQ"
      },
      "source": [
        "# Stemming words\n",
        "Removing unnecessary parts of words. <br>\n",
        "\n",
        "This process uses WordNetLemmatizer from nltk. This section:\n",
        "\n",
        "* Turns each summary in the \"Summary\" column into an array.\n",
        "* Makes every word lowercase.\n",
        "* Cycles through every possible tag in lemmatize (verb, noun, etc.).\n",
        "* Stops cycling when the word is stemmed.\n",
        "* Turns plural words into singular\n",
        "* Puts sentence back together.\n",
        "\n",
        "However, despite using different methods, some words still slip through."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "l4BjK5jBJJpZ"
      },
      "outputs": [],
      "source": [
        "tags = ['v','s','n','a','r']\n",
        "sentences = pd.Series(setsummaries)\n",
        "stem = WordNetLemmatizer()\n",
        "y = 0\n",
        "for sentence in sentences:\n",
        "  sentence = sentence.split(\" \") #Make arrays out of the sentences\n",
        "  x = 0\n",
        "  for word in sentence:\n",
        "    word = word.lower() #might take this out and do it earlier\n",
        "    for tag in tags:\n",
        "      lemma = stem.lemmatize(word,tag) #Stems the words and rotates 5 different tags for the lemmatizer\n",
        "      if len(lemma) < len(word):\n",
        "        break; #Breaks out of loop if the word gets shortened\n",
        "    lemma = singularize(lemma)\n",
        "    #putting the sentence back together\n",
        "    sentence[x] = lemma\n",
        "    x += 1\n",
        "  #putting the series of sentences back together\n",
        "  sentence = \" \".join(sentence)\n",
        "  sentences[y] = sentence\n",
        "  y += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUMSmcYvDxLB"
      },
      "source": [
        "# Transforming into bag of words / n-gram\n",
        "Use CountVectorizer() to drop stop words, and count the amount of times words appear. <br>\n",
        "* We change the parameter ngram_range to determine whether we want a bag of words model, or an n-gram. <br>\n",
        "\n",
        "When we start running models, then we will change the minimum occurances of words. <br>\n",
        "After that, we will add back the rest of the attributes to the bag of words for model usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF-mVI8fdASi"
      },
      "outputs": [],
      "source": [
        "#Getting ready to turn the summaries column into a bag of words\n",
        "moviesetstem['summaries'] = setsummaries\n",
        "moviesetstem.to_csv(\"stemmedmovieset.csv\",index = False)\n",
        "\n",
        "ngram = int(input('What number for n? (1 = bag of words model, 2+ = n-gram model): '))\n",
        "mindf = int(input('What should be the minimum occurances of words?: '))\n",
        "\n",
        "#Splitting Summaries, making a dictionary, forming it into a bag of words/n-gram\n",
        "CountSum = CountVectorizer(ngram_range=(ngram,ngram),stop_words='english', min_df = mindf)\n",
        "bow = CountSum.fit_transform(moviesetstem['summaries'])\n",
        "bagofwords = pd.DataFrame(bow.toarray(),columns=CountSum.get_feature_names())\n",
        "\n",
        "#removing the index\n",
        "bagofwords = bagofwords.reset_index(drop=True)\n",
        "moviesetcopy = moviesetstem.reset_index(drop=True)\n",
        "\n",
        "#Merging the datasets\n",
        "bagofwords['titles'] = moviesetcopy['titles']\n",
        "bagofwords = pd.merge(bagofwords, moviesetcopy, how = 'inner', on = 'titles').drop(columns = 'summaries', axis=1)\n",
        "\n",
        "bagofwords.to_csv('BagOfWords.csv',index = False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "IMDb Stemmer and Bag of Words",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}